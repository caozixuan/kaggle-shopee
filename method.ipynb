{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入必要的包\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml import PCA\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "import math\n",
    "from shutil import copyfile\n",
    "\n",
    "import tokenization\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import fasttext as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算F1 Score的函数\n",
    "def getMetric(col):\n",
    "    def f1score(row):\n",
    "        n = len( np.intersect1d(row.target,row[col]) )\n",
    "        return 2*n / (len(row.target)+len(row[col]))\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tf.dataset\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = [512, 512]\n",
    "# Seed\n",
    "SEED = 42\n",
    "# Verbosity\n",
    "VERBOSE = 1\n",
    "# Number of classes\n",
    "N_CLASSES = 11011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 所有配置内容\n",
    "class CFG:\n",
    "    \n",
    "    img_size = 512\n",
    "    batch_size = 12\n",
    "    seed = 2020\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes = 11014\n",
    "    \n",
    "    \n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5\n",
    "    \n",
    "    img_size = 512\n",
    "    fc_dim = 512\n",
    "    batch_size = 12\n",
    "    seed = 2020\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    classes = 11014\n",
    "    \n",
    "    model_name1 =  'tf_efficientnet_b4'\n",
    "    model_name2 = 'eca_nfnet_l0'\n",
    "    model_name3 = 'resnext50_32x4d'\n",
    "    model_name4 = 'tf_efficientnet_b5_ns'\n",
    "    model_name5 = 'efficientnet_b3'\n",
    "\n",
    "    model_path1 = './utils-shopee/arcface_512x512_tf_efficientnet_b4_LR.pt'\n",
    "    model_path2 = './shopee-pytorch-models/arcface_512x512_nfnet_l0 (mish).pt'\n",
    "    model_path3 = './shopee-pytorch-models/arcface_512x512_resnext32x4d.pt'\n",
    "    model_path4 = './shopee-pytorch-models/arcface_512x512_eff_b5_.pt'\n",
    "    model_path5 = './shopee-pytorch-models/arcface_512x512_eff_b3.pt'\n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 2.0\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
    "print('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to check ram allocations (debug)\n",
    "CHECK_SUB = False\n",
    "\n",
    "df = cudf.read_csv('./data/match/test.csv')\n",
    "# If we are comitting, replace train set for test set and dont get cv\n",
    "if len(df) > 3:\n",
    "    GET_CV = False\n",
    "del df\n",
    "GET_CV = False  # 决定出测试集数据还是训练集数据\n",
    "\n",
    "# Function to get f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Function to combine predictions\n",
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash']])\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "# Function to read dataset\n",
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('./data/match/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "        df['target'] = df.label_group.map(tmp)\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = './data/images/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('./data/match/test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = './data/images/test_images/' + df['image']\n",
    "        \n",
    "    return df, df_cu, image_paths\n",
    "\n",
    "\n",
    "# Function to decode our images\n",
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "# Function to read our test image and return image\n",
    "def read_image(image):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = decode_image(image)\n",
    "    return image\n",
    "\n",
    "# Function to get our dataset that read images\n",
    "def get_dataset(image):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image)\n",
    "    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mish 函数\n",
    "class Mish_func(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "  \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2)\n",
    "        \n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        #grad_hv = 1./v  \n",
    "        #grad_vx = i.exp()\n",
    "        \n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
    "        \n",
    "        return grad_output * grad_f \n",
    "    \n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "    \n",
    "    \n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    \n",
    "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
    "\n",
    "        if type(module) == existing_layer:\n",
    "            layer_old = module\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Arcmargin model\n",
    "class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    Implements large margin arc distance.\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/pdf/1801.07698.pdf\n",
    "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "            blob/master/src/modeling/metric_learning.py\n",
    "    '''\n",
    "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "                 ls_eps=0.0, **kwargs):\n",
    "\n",
    "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = tf.math.cos(m)\n",
    "        self.sin_m = tf.math.sin(m)\n",
    "        self.th = tf.math.cos(math.pi - m)\n",
    "        self.mm = tf.math.sin(math.pi - m) * m\n",
    "        \n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'ls_eps': self.ls_eps,\n",
    "            'easy_margin': self.easy_margin,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name='W',\n",
    "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "            initializer='glorot_uniform',\n",
    "            dtype='float32',\n",
    "            trainable=True,\n",
    "            regularizer=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        X, y = inputs\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        cosine = tf.matmul(\n",
    "            tf.math.l2_normalize(X, axis=1),\n",
    "            tf.math.l2_normalize(self.W, axis=0)\n",
    "        )\n",
    "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = tf.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        one_hot = tf.cast(\n",
    "            tf.one_hot(y, depth=self.n_classes),\n",
    "            dtype=cosine.dtype\n",
    "        )\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image embeddings for tensorflow\n",
    "def get_image_embeddings_tensorflow(image_paths):\n",
    "    embeds = []\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = N_CLASSES, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "    x = efn.EfficientNetB3(weights = None, include_top = False)(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = margin([x, label])\n",
    "        \n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "    model.load_weights('efficientnet/EfficientNetB3_512_42.h5')\n",
    "    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n",
    "    chunk = 5000\n",
    "    iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "    for j in iterator:\n",
    "        a = int(j * chunk)\n",
    "        b = int((j + 1) * chunk)\n",
    "        image_dataset = get_dataset(image_paths[a:b])\n",
    "        image_embeddings = model.predict(image_dataset)\n",
    "        embeds.append(image_embeddings)\n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings\n",
    "\n",
    "# Return tokens, masks and segments from a text array or series\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(df, max_len = 70):\n",
    "    embeds = []\n",
    "    module_url = \"bert-en-uncased-l24-h1024-a16-1\"\n",
    "    bert_layer = hub.KerasLayer(module_url, trainable = True)\n",
    "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n",
    "    \n",
    "    margin = ArcMarginProduct(\n",
    "            n_classes = 11014, \n",
    "            s = 30, \n",
    "            m = 0.5, \n",
    "            name='head/arc_margin', \n",
    "            dtype='float32'\n",
    "            )\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    x = margin([clf_output, label])\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
    "    \n",
    "    model.load_weights('./Bert_123.h5')\n",
    "    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n",
    "    chunk = 5000\n",
    "    iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "    for j in iterator:\n",
    "        a = int(j * chunk)\n",
    "        b = int((j + 1) * chunk)\n",
    "        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n",
    "        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n",
    "        embeds.append(text_embeddings)\n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(df, embeddings, KNN = 50, image = True):   \n",
    "    if len(df)<10:\n",
    "        KNN = 1\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "    if GET_CV:\n",
    "        if image:\n",
    "            thresholds = list(np.arange(4.5, 5.0, 0.1))\n",
    "        else:\n",
    "            thresholds = list(np.arange(31, 35, 1))\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            predictions = []\n",
    "            for k in range(embeddings.shape[0]):\n",
    "                idx = np.where(distances[k,] < threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "                predictions.append(posting_ids)\n",
    "            df['pred_matches'] = predictions\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        \n",
    "        # Use threshold\n",
    "        predictions = []\n",
    "        for k in range(embeddings.shape[0]):\n",
    "            ids = np.array([])\n",
    "            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 3.3)[0]\n",
    "                ids = indices[k,idx]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 20.0)[0]\n",
    "                ids = indices[k,idx]\n",
    "                if ((len)(idx)>1):\n",
    "                    arr = distances[k,np.where(distances[k,]<20)[0]][1:]\n",
    "                    mean = np.mean(arr)\n",
    "                    standard_deviation = np.std(arr)\n",
    "                    if(standard_deviation>0):\n",
    "                        distance_from_mean = abs(arr - mean)\n",
    "                        max_deviations = 2\n",
    "                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n",
    "                        max_dist = arr[not_outlier][-1]\n",
    "                        idx = np.where(distances[k,] <= max_dist)[0]\n",
    "                        ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "    \n",
    "    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "    else:\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            ids = np.array([])\n",
    "            if image:\n",
    "                idx = np.where(distances[k,] < 3.3)[0]\n",
    "                ids = indices[k,idx]\n",
    "            else:\n",
    "                idx = np.where(distances[k,] < 16.0)[0]\n",
    "                ids = indices[k,idx]\n",
    "                if (len(idx)>1):\n",
    "                    arr = distances[k,np.where(distances[k,]<16.0)[0]][1:]\n",
    "                    mean = np.mean(arr)\n",
    "                    standard_deviation = np.std(arr)\n",
    "                    if(standard_deviation>0):\n",
    "                        distance_from_mean = abs(arr - mean)\n",
    "                        max_deviations = 2\n",
    "                        not_outlier = distance_from_mean < max_deviations * standard_deviation\n",
    "                        max_dist = arr[not_outlier][-1]\n",
    "                        idx = np.where(distances[k,] <= max_dist)[0]\n",
    "                        ids = indices[k,idx]\n",
    "            \n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于Tensorflow模型拿到的结果\n",
    "df, df_cu, image_paths = read_dataset()\n",
    "image_embeddings = get_image_embeddings_tensorflow(image_paths)\n",
    "text_embeddings = get_text_embeddings(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProductTorch(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProductTorch, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "\n",
    "        return output\n",
    "    \n",
    "class ShopeeModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = CFG.classes,\n",
    "        model_name = CFG.model_name,\n",
    "        fc_dim = 512,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = False,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(ShopeeModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == 'resnext50_32x4d':\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'efficientnet_b3':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'tf_efficientnet_b5_ns':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        elif model_name == 'nfnet_f3':\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self._init_params()\n",
    "        final_in_features = fc_dim\n",
    "\n",
    "        self.final = ArcMarginProductTorch(\n",
    "            final_in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        #logits = self.final(feature,label)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings_torch(image_paths, model_name = CFG.model_name1, model_path = CFG.model_path1):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeModel(model_name = model_name)\n",
    "    model.eval()\n",
    "    \n",
    "    if model_name == 'eca_nfnet_l0':\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(CFG.device)\n",
    "    \n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    \n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)\n",
    "    \n",
    "def get_test_transforms():\n",
    "\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
    "            A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions_torch(df, embeddings1, embeddings4, threshold = 3.4):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 1\n",
    "    \n",
    "    #--\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings1)\n",
    "    distances, indices = model.kneighbors(embeddings1)\n",
    "    \n",
    "    threshold = 2.2  # 1。7\n",
    "    predictions1 = []\n",
    "    for k in tqdm(range(embeddings1.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = list(df['posting_id'].iloc[ids])\n",
    "        predictions1.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices, embeddings1\n",
    "    gc.collect()\n",
    "\n",
    "    #--\n",
    "    \"\"\"\n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
    "    model.fit(embeddings3)\n",
    "    distances, indices = model.kneighbors(embeddings3)\n",
    "    \n",
    "    threshold=0.36   # 0.36\n",
    "    predictions3 = []\n",
    "    for k in tqdm(range(embeddings3.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = list(df['posting_id'].iloc[ids])\n",
    "        predictions3.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices, embeddings3\n",
    "    gc.collect()\n",
    "    \"\"\"\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings4)\n",
    "    distances, indices = model.kneighbors(embeddings4)\n",
    "    threshold = 5.2   # 4.5\n",
    "    predictions4 = []\n",
    "    \n",
    "    for k in tqdm(range(embeddings4.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = list(df['posting_id'].iloc[ids])\n",
    "        predictions4.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices, embeddings4\n",
    "    gc.collect()\n",
    "    \n",
    "         \n",
    "    predictions = [list(set(a +b)) for a, b in zip(predictions1, predictions4)]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, image_predictions = get_neighbors(df, image_embeddings, KNN = 25, image = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, text_predictions = get_neighbors(df, text_embeddings, KNN = 25, image = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "model = TfidfVectorizer(stop_words=None, binary=True, max_features=25000)\n",
    "text_embeddings2 = model.fit_transform(df_cu.title).toarray()\n",
    "print('text embeddings shape',text_embeddings2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "CHUNK = 1024*4\n",
    "\n",
    "print('Finding similar titles...')\n",
    "CTS = len(df_cu)//CHUNK\n",
    "if len(df_cu)%CHUNK!=0: CTS += 1\n",
    "for j in range( CTS ):\n",
    "    \n",
    "    a = j*CHUNK\n",
    "    b = (j+1)*CHUNK\n",
    "    b = min(b,len(df_cu))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    # COSINE SIMILARITY DISTANCE\n",
    "    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n",
    "    cts = cupy.matmul(text_embeddings2, text_embeddings2[a:b].T).T\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        # IDX = np.where(cts[k,]>0.7)[0]\n",
    "        IDX = cupy.where(cts[k,]>0.75)[0]\n",
    "        o = df_cu.iloc[cupy.asnumpy(IDX)].posting_id.to_pandas().values\n",
    "        preds.append(o)\n",
    "        \n",
    "del model, text_embeddings2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cu['oof_text'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "    df_cu = cudf.DataFrame(df)\n",
    "    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "    return df, df_cu, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings,threshold = 0.0):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in tqdm(range(embeddings.shape[0])):\n",
    "        idx = np.where(distances[k,] < threshold)[0]\n",
    "        ids = indices[k,idx]\n",
    "        posting_ids = df['posting_id'].iloc[ids].values\n",
    "        predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image,df_image_cu,image_paths = read_dataset()\n",
    "df_image.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_tensorflow1 = get_image_embeddings_tensorflow(image_paths.values)\n",
    "image_predictions_tensorflow = get_image_predictions(df_image, image_embeddings1, threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings_torch1= get_image_embeddings_torch(image_paths.values, CFG.model_name1,CFG.model_path1)\n",
    "image_embeddings_torch4 = get_image_embeddings_torch(image_paths.values, CFG.model_name4,CFG.model_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions_torch = get_image_predictions_torch(df_image, image_embeddings_torch1, image_embeddings_torch4, threshold = 1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash']])\n",
    "    return ' '.join( np.unique(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_predictions_submit(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'], row['oof_text'], row['oof_hash'],row['oof_image'],row['image_predictions_new']])\n",
    "    return ' '.join( np.unique(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "df['oof_hash'] = df.image_phash.map(tmp)\n",
    "if GET_CV:\n",
    "    df['image_predictions'] = image_predictions_tensorflow1\n",
    "    #df['image_predictions_new'] = image_predictions_new\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n",
    "    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "    score = df['f1'].mean()\n",
    "    print(f'Our final f1 cv score is {score}')\n",
    "    df['matches'] = df['pred_matches']\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    df['image_predictions_new'] = image_predictions_torch\n",
    "    df['image_predictions'] = image_predictions_tensorflow\n",
    "    df['oof_text'] = df_cu['oof_text'].to_pandas().values\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['oof_image']=image_predictions1\n",
    "    df['matches'] = df.apply(combine_predictions_submit, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
